{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as nn_init\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import typing as ty\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "generate\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tokenizer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_numerical, categories, d_token, bias):\n",
    "        super().__init__()\n",
    "        if categories is None:\n",
    "            d_bias = d_numerical\n",
    "            self.category_offsets = None\n",
    "            self.category_embeddings = None\n",
    "        else:\n",
    "            d_bias = d_numerical + len(categories)\n",
    "            category_offsets = torch.tensor([0] + categories[:-1]).cumsum(0)\n",
    "            self.register_buffer('category_offsets', category_offsets)\n",
    "            self.category_embeddings = nn.Embedding(sum(categories), d_token)\n",
    "            nn_init.kaiming_uniform_(self.category_embeddings.weight, a=math.sqrt(5))\n",
    "            print(f'{self.category_embeddings.weight.shape=}')\n",
    "\n",
    "        # take [CLS] token into account\n",
    "        self.weight = nn.Parameter(Tensor(d_numerical + 1, d_token))\n",
    "        self.bias = nn.Parameter(Tensor(d_bias, d_token)) if bias else None\n",
    "        # The initialization is inspired by nn.Linear\n",
    "        nn_init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            nn_init.kaiming_uniform_(self.bias, a=math.sqrt(5))\n",
    "\n",
    "    @property\n",
    "    def n_tokens(self):\n",
    "        return len(self.weight) + (\n",
    "            0 if self.category_offsets is None else len(self.category_offsets)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        x_some = x_num if x_cat is None else x_cat\n",
    "        assert x_some is not None\n",
    "        x_num = torch.cat(\n",
    "            [torch.ones(len(x_some), 1, device=x_some.device)]  # [CLS]\n",
    "            + ([] if x_num is None else [x_num]),\n",
    "            dim=1,\n",
    "        )\n",
    "    \n",
    "        x = self.weight[None] * x_num[:, :, None]\n",
    "\n",
    "        if x_cat is not None:\n",
    "            x = torch.cat(\n",
    "                [x, self.category_embeddings(x_cat + self.category_offsets[None])],\n",
    "                dim=1,\n",
    "            )\n",
    "        if self.bias is not None:\n",
    "            bias = torch.cat(\n",
    "                [\n",
    "                    torch.zeros(1, self.bias.shape[1], device=x.device),\n",
    "                    self.bias,\n",
    "                ]\n",
    "            )\n",
    "            x = x + bias[None]\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train = np.load('/mnt/nas/swethamagesh/tabsyn-fresh/tabsyn/data/adult_cond/X_cat_train.npy', allow_pickle=True)\n",
    "num_train = np.load('/mnt/nas/swethamagesh/tabsyn-fresh/tabsyn/data/adult_cond/X_num_train.npy')\n",
    "target = np.load('/mnt/nas/swethamagesh/tabsyn-fresh/tabsyn/data/adult_cond/y_train.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cat_train = np.concatenate([target, cat_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([50, 4])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(5, [3, 7, 16, 5, 7, 5, 3, 2, 2], 4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 4, 10, ..., 0, 1, 1],\n",
       "       [0, 4, 9, ..., 0, 0, 1],\n",
       "       [0, 1, 9, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 5, 9, ..., 0, 0, 1],\n",
       "       [0, 4, 10, ..., 0, 0, 1],\n",
       "       [0, 4, 10, ..., 0, 0, 1]], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_cat_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  4., 10.,  ...,  0.,  1.,  1.],\n",
       "        [ 0.,  4.,  9.,  ...,  0.,  0.,  1.],\n",
       "        [ 0.,  1.,  9.,  ...,  0.,  0.,  1.],\n",
       "        ...,\n",
       "        [ 0.,  5.,  9.,  ...,  0.,  0.,  1.],\n",
       "        [ 0.,  4., 10.,  ...,  0.,  0.,  1.],\n",
       "        [ 0.,  4., 10.,  ...,  0.,  0.,  1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(final_cat_train.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31062, 15, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_out = tokenizer(torch.from_numpy(num_train), torch.from_numpy(final_cat_train.astype(int)))\n",
    "tokenized_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30k \n",
    "#  get tokenized output\n",
    "# (generate constraint for the entire set at once) - [111100011] - c\n",
    "# constraint for row - 0  & 1 - 0.4  - masked from tokenizd output cxd \n",
    "\n",
    "\n",
    "'''\n",
    "Model load from f'{ckpt_dir}/model.pt'\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt/nas/swethamagesh/ORD/restsyn/tabsyn/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from vae.model import Model_VAE, VAE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([50, 4])\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "WD = 0\n",
    "D_TOKEN = 4\n",
    "TOKEN_BIAS = True\n",
    "\n",
    "N_HEAD = 1\n",
    "FACTOR = 32\n",
    "NUM_LAYERS = 2\n",
    "d_numerical = 5\n",
    "categories = [3, 7, 16, 5, 7, 5, 3, 2, 2]\n",
    "model_vae = Model_VAE(NUM_LAYERS, d_numerical, categories, D_TOKEN, n_head = N_HEAD, factor = FACTOR, bias = True)\n",
    "model_vae.load_state_dict(torch.load('/mnt/nas/swethamagesh/tabsyn-fresh/tabsyn/tabsyn/vae/ckpt/adult_cond/model.pt', weights_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_out = model_vae.VAE.Tokenizer(torch.from_numpy(num_train), torch.from_numpy(final_cat_train.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 0, 1, 0],\n",
       "       [1, 0, 0, ..., 1, 0, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 0, 1, ..., 0, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "prob of masking = 0.5 to 1 \n",
    "\n",
    "'''\n",
    "p = 0.5\n",
    "q = 1\n",
    "N = 31062\n",
    "cols = 15\n",
    "# sample between p & q for N samples\n",
    "mask = np.random.uniform(p, q, N)\n",
    "\n",
    "# generate a 2d binary mask with mask prob for each column (Nxcols)\n",
    "# given mask value for a given row, mask each column in the tokenized output with that prob\n",
    "mask_2d = np.random.binomial(1, mask[:, None], (N, cols))\n",
    "mask_3d = np.stack([mask_2d for i in range(4)], axis=2)\n",
    "constraint_train = tokenized_out.detach().numpy() * mask_3d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file = constraint_train.reshape(constraint_train.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.80650026e-01,  4.39514816e-01, -3.10437918e-01,\n",
       "        -2.80084968e-01],\n",
       "       [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00],\n",
       "       [-6.26748535e+03,  4.29589492e+04, -8.94396250e+04,\n",
       "         2.23192422e+04],\n",
       "       [ 3.01283717e-01,  3.67014587e-01, -5.44309579e-02,\n",
       "         3.80284965e-01],\n",
       "       [-2.42611572e-01, -1.42419353e-01, -1.85007319e-01,\n",
       "         3.61240894e-01],\n",
       "       [ 2.78375769e+00,  3.80989718e+00, -1.02589111e+01,\n",
       "         4.47270346e+00],\n",
       "       [ 6.11785054e-01, -1.37224212e-01,  2.17554674e-01,\n",
       "         2.40170389e-01],\n",
       "       [-0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00],\n",
       "       [-0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00],\n",
       "       [ 3.45189750e-01, -3.02160472e-01,  4.57613111e-01,\n",
       "        -4.28387493e-01],\n",
       "       [-2.29718685e-01, -2.84911990e-01, -4.06368732e-01,\n",
       "        -2.96661705e-01],\n",
       "       [-0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-7.05517471e-01,  3.89568835e-01,  3.01005483e-01,\n",
       "         6.39429748e-01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraint_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.80650026e-01,  4.39514816e-01, -3.10437918e-01, -2.80084968e-01,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -6.26748535e+03,  4.29589492e+04, -8.94396250e+04,  2.23192422e+04,\n",
       "        3.01283717e-01,  3.67014587e-01, -5.44309579e-02,  3.80284965e-01,\n",
       "       -2.42611572e-01, -1.42419353e-01, -1.85007319e-01,  3.61240894e-01,\n",
       "        2.78375769e+00,  3.80989718e+00, -1.02589111e+01,  4.47270346e+00,\n",
       "        6.11785054e-01, -1.37224212e-01,  2.17554674e-01,  2.40170389e-01,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        3.45189750e-01, -3.02160472e-01,  4.57613111e-01, -4.28387493e-01,\n",
       "       -2.29718685e-01, -2.84911990e-01, -4.06368732e-01, -2.96661705e-01,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -7.05517471e-01,  3.89568835e-01,  3.01005483e-01,  6.39429748e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_file[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/mnt/nas/swethamagesh/ORD/restsyn/data/adult_cond/constraints.npy', final_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umpteenth_attempt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
